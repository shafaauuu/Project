{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from collections import defaultdict\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "print(\"Importing libraries...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('said')\n",
    "stop_words.add('mr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = 'E:/Natural Language Processing/Project/ML/dataset'\n",
    "LABELS = ['business', 'entertainment', 'food', 'graphics', 'historical', 'medical'\n",
    "          , 'politics', 'space', 'sport', 'technologies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set():\n",
    "    with open('data.txt', 'w', encoding='utf8') as outfile:\n",
    "        for label in LABELS:\n",
    "            dir = '%s%s' % (BASE_DIR, label)\n",
    "            for filename in os.listdir(dir):\n",
    "                fullfilename = '%s%s' % (dir, filename)\n",
    "                print(fullfilename)\n",
    "                with open(fullfilename, 'rb') as file:\n",
    "                    text = file.read().decode(errors='replace').replace('\\n', '')\n",
    "                    outfile.write('%s\\t%s\\t%s\\n' % (label, filename, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_docs():\n",
    "    docs = [] # (label, text)\n",
    "    with open('data.txt', 'w', encoding='utf8') as datafile:\n",
    "        for row in datafile:\n",
    "            parts = row.split('\\t')\n",
    "            doc = (parts[0], parts[2].strip())\n",
    "\n",
    "            docs.append(doc)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(text):\n",
    "    #get individual words\n",
    "    tokens = word_tokenize(text)\n",
    "    #remove common useless words\n",
    "    tokens = [t for t in tokens if not t in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frequency_dist(docs):\n",
    "    tokens = defaultdict(list)\n",
    "\n",
    "    for doc in docs:\n",
    "        doc_label = doc[0] \n",
    "\n",
    "        doc_text = clean_text(doc[1])\n",
    "\n",
    "        doc_tokens = word_tokenize(doc_text)\n",
    "\n",
    "        tokens[doc_label].extend(doc_tokens)\n",
    "\n",
    "    for category_label, category_tokens in tokens.items():\n",
    "        print(category_label)\n",
    "        fd = FreqDist(category_tokens)\n",
    "        print(fd.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(docs):\n",
    "    random.shuffle(docs)\n",
    "\n",
    "    X_train =  []\n",
    "    y_train = []\n",
    "\n",
    "    X_test = []\n",
    "    y_test = []\n",
    "\n",
    "    pivot = int(.80 * len(docs))\n",
    "\n",
    "    for i in range(0, pivot):\n",
    "        X_train.append(docs[i][1])\n",
    "        y_train.append(docs[i][0])\n",
    "\n",
    "    for i in range(pivot, len(docs)):\n",
    "        X_test.append(docs[i][1])\n",
    "        y_test.append(docs[i][0])\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(title, classifier, vectorizer, X_test, y_test):\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "    precision = metrics.precision_score(y_test, y_pred)\n",
    "    recall = metrics.recall_score(y_test, y_pred)\n",
    "    f1 = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "    print(\"%s\\tPrecision: %f\\tRecall: %f\\tF1 Score: %f\\n\" % (title, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(docs):\n",
    "    X_train, X_test, y_train, y_test = get_splits(docs)\n",
    "\n",
    "    vectorizer = CountVectorizer(stop_words='english',\n",
    "                                 ngram_range=(1, 3),\n",
    "                                 min_df=3, analyzer='word')\n",
    "\n",
    "    dtm = vectorizer.fit_transform(X_train)\n",
    "\n",
    "    #train naive bayes classifier\n",
    "    naive_bayes_classifier = MultinomialNB().fit(dtm, y_train)\n",
    "\n",
    "    evaluate_classifier(\"Naive Bayes\\tTRAIN\\t\", naive_bayes_classifier, vectorizer, X_train, y_train)\n",
    "    evaluate_classifier(\"Naive  Bayes\\tTEST\\t\", naive_bayes_classifier, vectorizer, X_test, y_test)\n",
    "\n",
    "    #storing\n",
    "    clf_filename = 'naive_bayes_classifier.pkl'\n",
    "    pickle.dump(naive_bayes_classifier, open(clf_filename, 'wb'))\n",
    "\n",
    "    vec_filename = 'count_vectorizer.pkl'\n",
    "    pickle.dump(vectorizer, open(vec_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    clf_filename = 'E:/Natural Language Processing/Project/ML/dataset/naive_bayes_classifier.pkl'\n",
    "    nb_clf = pickle.load(open(clf_filename, 'rb'))\n",
    "\n",
    "    vec_filename = 'E:/Natural Language Processing/Project/ML/dataset/count_vectorizer.pkl'\n",
    "    vectorizer = pickle.load(open(vec_filename, 'rb'))\n",
    "\n",
    "    pred = nb_clf.predict(vectorizer.transform([text]))\n",
    "\n",
    "    print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:/Natural Language Processing/Project/ML/dataset/naive_bayes_classifier.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m#create_data_set()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m#docs = setup_docs()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m#train_classifier(docs)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     new_doc \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the text to classify: A new California restaurant claims to be the first fully autonomous restaurant, with its burgers and fries made by robots. As fast food chains increasingly try to find ways for machines to replace some human workers, NBC News’ Elwyn Lopez gets an inside look.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[43mclassify\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_doc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prediction)\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[18], line 3\u001b[0m, in \u001b[0;36mclassify\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclassify\u001b[39m(text):\n\u001b[0;32m      2\u001b[0m     clf_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/Natural Language Processing/Project/ML/dataset/naive_bayes_classifier.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     nb_clf \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mclf_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m     vec_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE:/Natural Language Processing/Project/ML/dataset/count_vectorizer.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m     vectorizer \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mopen\u001b[39m(vec_filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'E:/Natural Language Processing/Project/ML/dataset/naive_bayes_classifier.pkl'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #create_data_set()\n",
    "    #docs = setup_docs()\n",
    "\n",
    "    #print_frequency_dist(docs)\n",
    "\n",
    "    #train_classifier(docs)\n",
    "\n",
    "    new_doc = (\"Enter the text to classify: A new California restaurant claims to be the first fully autonomous restaurant, with its burgers and fries made by robots. As fast food chains increasingly try to find ways for machines to replace some human workers, NBC News’ Elwyn Lopez gets an inside look.\")\n",
    "\n",
    "    prediction = classify(new_doc)\n",
    "\n",
    "    print(\"Classification result:\", prediction)\n",
    "\n",
    "    print(\"All Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
